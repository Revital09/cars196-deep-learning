{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23dba62d-7ebf-44d0-86b2-0e74283f27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# Import required libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01059efa-5ee7-45b1-9b28-f16978a4225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarsDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None):\n",
    "        self.img_paths = [os.path.join(img_dir, sample[-1][0]) for sample in annotations[0]]\n",
    "        self.labels = [sample[-2][0][0] - 1 for sample in annotations[0]]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "cars_dataset = torch.load(\"cars196_dataset.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb9c8cf-a2c2-4bbf-87d3-aa7060689eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "מספר דוגמאות בדאטהסט: 16185\n"
     ]
    }
   ],
   "source": [
    "# הצגת גודל הדאטהסט\n",
    "print(f\"מספר דוגמאות בדאטהסט: {len(cars_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce846e73-83ff-4dd0-be20-fda4cf7fab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarsDatasetWrapper(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.subset[idx]  # שליפת נתונים מה-Subset\n",
    "        if self.transform:\n",
    "            if isinstance(image, torch.Tensor):  # **בדיקה אם התמונה כבר טנסור**\n",
    "                pass  # אם זה כבר טנסור, לא צריך להפעיל שוב את `ToTensor()`\n",
    "            else:\n",
    "                image = self.transform(image)  # **הוספת האוגמנטציה**\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f3f87f6-530b-453c-a5b0-d2538321b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# הגדרת טרנספורמציות לתמונות\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # שינוי גודל כל התמונות ל-224x224 פיקסלים\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # היפוך אופקי של התמונה בהסתברות של 50%\n",
    "    transforms.RandomRotation(15),  # סיבוב אקראי של התמונה עד 15 מעלות לכל כיוון\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  # שינוי אקראי של הבהירות, הניגודיות, הרווייה והגוון של התמונה\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)), # תזוזה אקראית של התמונה עד 20% מהגודל שלה (לרוחב ולאורך)\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.5),  # עיוות פרספקטיבה בהסתברות של 50%, עם עיוות מקסימלי של 30%\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),  # חיתוך אקראי מהתמונה תוך שמירה על יחס גובה-רוחב, עם קנה מידה בין 70% ל-100% מהגודל המקורי\n",
    "    transforms.ToTensor(),  # המרת התמונה ל- **טנסור של PyTorch** (C, H, W) עם טווח ערכים בין 0 ל-1\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # נרמול התמונה לערכים עם ממוצע וסטיית תקן כמו של **ImageNet**, לשיפור האימון\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffadf586-ec92-4a22-92ea-990fb57e4008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11329, Testing samples: 4856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# חלוקת הדאטה לאימון ולמבחן\n",
    "dataset_size = len(cars_dataset)\n",
    "train_size = int(0.7 * dataset_size)  # 70% מהדאטה\n",
    "test_size = dataset_size - train_size  # 30% הנותרים\n",
    "\n",
    "train_indices, test_indices = train_test_split(range(dataset_size), train_size=train_size, test_size=test_size, random_state=42)\n",
    "\n",
    "# יצירת סטים עם אוגמנטציה\n",
    "train_dataset = Subset(cars_dataset, train_indices)\n",
    "test_dataset = Subset(cars_dataset, test_indices)\n",
    "\n",
    "# עטיפת ה-Subset עם הטרנספורמציות\n",
    "train_dataset = CarsDatasetWrapper(train_dataset, transform=train_transform)\n",
    "test_dataset = CarsDatasetWrapper(test_dataset, transform=test_transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "# יצירת DataLoader עם סטים מעודכנים\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68fd0c0-b3c6-404a-9e5c-3cfdae0e5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "base_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d1bd3df-a17d-4320-8bb6-042996e1a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMobileNet(nn.Module):\n",
    "    def __init__(self, base_model, num_classes=196):\n",
    "        super(CustomMobileNet, self).__init__()\n",
    "        self.base = nn.Sequential(*list(base_model.children())[:-2])  # שמירה על כל השכבות של \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # pooling\n",
    "        self.dropout = nn.Dropout(0.5)  # הוסף שכבת Dropout\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten לפני fully connected layers\n",
    "        x = self.dropout(x)  # Dropout לפני השכבה הסופית\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# יצירת המודל עם 196 מחלקות\n",
    "model = CustomMobileNet(base_model, num_classes=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32f65bc7-aa40-4868-8de0-844eeb28394d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomMobileNet(\n",
       "  (base): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=2048, out_features=196, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b988e316-1418-4a10-84a6-45ee1483d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 196])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)  # תמונה מדומה\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)  # צריך להיות torch.Size([1, 196])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1370c3f7-7ca7-4227-b4b3-32f4379becf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Trainable\n",
       "=================================================================================================================================================\n",
       "CustomMobileNet                               [1, 3, 224, 224]          [1, 196]                  --                        True\n",
       "├─Sequential: 1-1                             [1, 3, 224, 224]          [1, 2048, 7, 7]           --                        True\n",
       "│    └─Conv2d: 2-1                            [1, 3, 224, 224]          [1, 64, 112, 112]         9,408                     True\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 112, 112]         [1, 64, 112, 112]         128                       True\n",
       "│    └─ReLU: 2-3                              [1, 64, 112, 112]         [1, 64, 112, 112]         --                        --\n",
       "│    └─MaxPool2d: 2-4                         [1, 64, 112, 112]         [1, 64, 56, 56]           --                        --\n",
       "│    └─Sequential: 2-5                        [1, 64, 56, 56]           [1, 256, 56, 56]          --                        True\n",
       "│    │    └─Bottleneck: 3-1                   [1, 64, 56, 56]           [1, 256, 56, 56]          75,008                    True\n",
       "│    │    └─Bottleneck: 3-2                   [1, 256, 56, 56]          [1, 256, 56, 56]          70,400                    True\n",
       "│    │    └─Bottleneck: 3-3                   [1, 256, 56, 56]          [1, 256, 56, 56]          70,400                    True\n",
       "│    └─Sequential: 2-6                        [1, 256, 56, 56]          [1, 512, 28, 28]          --                        True\n",
       "│    │    └─Bottleneck: 3-4                   [1, 256, 56, 56]          [1, 512, 28, 28]          379,392                   True\n",
       "│    │    └─Bottleneck: 3-5                   [1, 512, 28, 28]          [1, 512, 28, 28]          280,064                   True\n",
       "│    │    └─Bottleneck: 3-6                   [1, 512, 28, 28]          [1, 512, 28, 28]          280,064                   True\n",
       "│    │    └─Bottleneck: 3-7                   [1, 512, 28, 28]          [1, 512, 28, 28]          280,064                   True\n",
       "│    └─Sequential: 2-7                        [1, 512, 28, 28]          [1, 1024, 14, 14]         --                        True\n",
       "│    │    └─Bottleneck: 3-8                   [1, 512, 28, 28]          [1, 1024, 14, 14]         1,512,448                 True\n",
       "│    │    └─Bottleneck: 3-9                   [1, 1024, 14, 14]         [1, 1024, 14, 14]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-10                  [1, 1024, 14, 14]         [1, 1024, 14, 14]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-11                  [1, 1024, 14, 14]         [1, 1024, 14, 14]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-12                  [1, 1024, 14, 14]         [1, 1024, 14, 14]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-13                  [1, 1024, 14, 14]         [1, 1024, 14, 14]         1,117,184                 True\n",
       "│    └─Sequential: 2-8                        [1, 1024, 14, 14]         [1, 2048, 7, 7]           --                        True\n",
       "│    │    └─Bottleneck: 3-14                  [1, 1024, 14, 14]         [1, 2048, 7, 7]           6,039,552                 True\n",
       "│    │    └─Bottleneck: 3-15                  [1, 2048, 7, 7]           [1, 2048, 7, 7]           4,462,592                 True\n",
       "│    │    └─Bottleneck: 3-16                  [1, 2048, 7, 7]           [1, 2048, 7, 7]           4,462,592                 True\n",
       "├─AdaptiveAvgPool2d: 1-2                      [1, 2048, 7, 7]           [1, 2048, 1, 1]           --                        --\n",
       "├─Dropout: 1-3                                [1, 2048]                 [1, 2048]                 --                        --\n",
       "├─Linear: 1-4                                 [1, 2048]                 [1, 196]                  401,604                   True\n",
       "=================================================================================================================================================\n",
       "Total params: 23,909,636\n",
       "Trainable params: 23,909,636\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.09\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.83\n",
       "Params size (MB): 95.64\n",
       "Estimated Total Size (MB): 274.07\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5050dd11-95ff-4804-8721-4012a55bb54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Size: 11329\n",
      "Test Loader Size: 4856\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Loader Size: {len(train_loader.dataset)}\")\n",
    "print(f\"Test Loader Size: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e22530e-be82-49e5-9561-845fced7c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b93064f1-cba3-4da5-a84c-5e0a780bd3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    print(\"Starting Training Epoch...\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        target = target.to(device).long()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss /= len(train_loader)\n",
    "    \n",
    "    print('Train Loss:', running_loss)\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_loss = 0\n",
    "        total_predictions = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device).long()\n",
    "            outputs = model(data)  # forward step\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs, target)  # loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions / total_predictions) * 100.0\n",
    "        print('Test Loss:', running_loss, 'Accuracy:', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d54cb7fe-d0ff-4ba5-b73c-4014a2c4f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_train = 8\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # מקפיא את כל השכבות\n",
    "\n",
    "for param in list(model.base.parameters())[-layers_to_train:]:  \n",
    "    param.requires_grad = True  # פותח השכבות האחרונות לאימון"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7548e81-d812-456a-9019-f681a5acdc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Epoch...\n",
      "Train Loss: 4.897201775832915\n",
      "Test Loss: 4.387911851468839 Accuracy: 15.074135090609555 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 4.239113963489801\n",
      "Test Loss: 3.968842498565975 Accuracy: 24.6499176276771 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 3.894526634081988\n",
      "Test Loss: 3.7164823632491264 Accuracy: 32.37232289950577 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 3.6127395005293295\n",
      "Test Loss: 3.5280948626367667 Accuracy: 35.728995057660626 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 3.380571442590633\n",
      "Test Loss: 3.330785975644463 Accuracy: 39.90939044481054 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 3.146136897046801\n",
      "Test Loss: 3.171225632491865 Accuracy: 41.37149917627677 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 2.931519355908246\n",
      "Test Loss: 3.035189661540483 Accuracy: 43.94563426688632 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 2.7226700903664174\n",
      "Test Loss: 2.9816756483755613 Accuracy: 44.254530477759474 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 2.5279774561734265\n",
      "Test Loss: 2.790888276539351 Accuracy: 48.167215815485996 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 2.334776729597172\n",
      "Test Loss: 2.715846184052919 Accuracy: 47.38467874794069 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 2.1586095531221847\n",
      "Test Loss: 2.661650885876856 Accuracy: 49.25864909390445 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 1.9925072643118844\n",
      "Test Loss: 2.521529782759516 Accuracy: 50.391268533772646 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 1.8228266148500039\n",
      "Test Loss: 2.5423904053474726 Accuracy: 50.02059308072487 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 1.65779573548008\n",
      "Test Loss: 2.448909677172962 Accuracy: 50.24711696869851 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 1.52292920586089\n",
      "Test Loss: 2.3922170400619507 Accuracy: 51.40032948929159 %\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "try:\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler)  \n",
    "        test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "        \n",
    "        Train_loss.append(train_loss)\n",
    "        Test_loss.append(test_loss)\n",
    "        Test_acc.append(test_acc)\n",
    "        \n",
    "        print('*' * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during training:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f4a1349-0e1e-4202-b5b2-f2625c0d6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_train = 10\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # מקפיא את כל השכבות\n",
    "\n",
    "for param in list(model.base.parameters())[-layers_to_train:]:  \n",
    "    param.requires_grad = True  # פותח השכבות האחרונות לאימון"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1da5e3a5-66e4-47b6-b529-c75af3545085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Epoch...\n",
      "Train Loss: 1.6324496685619085\n",
      "Test Loss: 2.135765147836585 Accuracy: 53.95387149917627 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 1.1515196989959395\n",
      "Test Loss: 2.0455606760163056 Accuracy: 56.610378912685334 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.8661039497650845\n",
      "Test Loss: 2.035429734932749 Accuracy: 56.05436573311368 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.6570340656898391\n",
      "Test Loss: 1.9902351686829014 Accuracy: 59.65815485996705 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.4899776259778251\n",
      "Test Loss: 1.850536676613908 Accuracy: 61.28500823723228 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.37628881318468443\n",
      "Test Loss: 1.814874593364565 Accuracy: 62.25288303130149 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.2821232030089472\n",
      "Test Loss: 1.8158671604959589 Accuracy: 62.35584843492587 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.23629684465032227\n",
      "Test Loss: 1.8790309546809447 Accuracy: 61.635090609555185 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.18576693509666012\n",
      "Test Loss: 1.779530026802891 Accuracy: 64.97116968698518 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.14202472900210972\n",
      "Test Loss: 1.7737467171330201 Accuracy: 63.85914332784185 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.12512589192516368\n",
      "Test Loss: 1.82656975717921 Accuracy: 64.41515650741351 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.11524937754155884\n",
      "Test Loss: 1.7649815568798466 Accuracy: 64.06507413509061 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.0962392575828962\n",
      "Test Loss: 1.7740205447924764 Accuracy: 64.64168039538714 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.08941404606464882\n",
      "Test Loss: 1.7878773714366711 Accuracy: 65.15650741350906 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.08103386234544532\n",
      "Test Loss: 1.7540358446146314 Accuracy: 65.56836902800659 %\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "try:\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler)  \n",
    "        test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "        \n",
    "        Train_loss.append(train_loss)\n",
    "        Test_loss.append(test_loss)\n",
    "        Test_acc.append(test_acc)\n",
    "        \n",
    "        print('*' * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during training:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da96150-88b8-4205-8703-804bb17205fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19dc90cb-2746-4cfb-a888-027175ce5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"test_18.03.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4773f3dd-3327-4cae-8b0a-56eae014405b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fb710da-cb82-4840-b4e6-885afe745c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Epoch...\n",
      "Train Loss: 0.06971881735912511\n",
      "Test Loss: 1.721673428228027 Accuracy: 67.5658978583196 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n",
      "Train Loss: 0.06872709143224737\n",
      "Test Loss: 1.842686434325419 Accuracy: 64.39456342668863 %\n",
      "************************************************************\n",
      "Starting Training Epoch...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m----> 4\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer, scheduler)  \n\u001b[0;32m      5\u001b[0m         test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, criterion)\n\u001b[0;32m      7\u001b[0m         Train_loss\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[30], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, scheduler)\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)\n\u001b[0;32m     12\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m, in \u001b[0;36mCustomMobileNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase(x)\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_avg_pool(x)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten לפני fully connected layers\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[1;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    202\u001b[0m     bn_training,\n\u001b[0;32m    203\u001b[0m     exponential_average_factor,\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2813\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2814\u001b[0m     weight,\n\u001b[0;32m   2815\u001b[0m     bias,\n\u001b[0;32m   2816\u001b[0m     running_mean,\n\u001b[0;32m   2817\u001b[0m     running_var,\n\u001b[0;32m   2818\u001b[0m     training,\n\u001b[0;32m   2819\u001b[0m     momentum,\n\u001b[0;32m   2820\u001b[0m     eps,\n\u001b[0;32m   2821\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled,\n\u001b[0;32m   2822\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "try:\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler)  \n",
    "        test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "        \n",
    "        Train_loss.append(train_loss)\n",
    "        Test_loss.append(test_loss)\n",
    "        Test_acc.append(test_acc)\n",
    "        \n",
    "        print('*' * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during training:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8067e3e-f583-4220-9f0f-9adc7d3b0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_train = 12\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # מקפיא את כל השכבות\n",
    "\n",
    "for param in list(model.base.parameters())[-layers_to_train:]:  \n",
    "    param.requires_grad = True  # פותח השכבות האחרונות לאימון"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b13a2f-006d-4fc5-b99c-ac6fb94bd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "try:\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler)  \n",
    "        test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "        \n",
    "        Train_loss.append(train_loss)\n",
    "        Test_loss.append(test_loss)\n",
    "        Test_acc.append(test_acc)\n",
    "        \n",
    "        print('*' * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during training:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cc227-f109-4166-9e9b-2396efefc068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
